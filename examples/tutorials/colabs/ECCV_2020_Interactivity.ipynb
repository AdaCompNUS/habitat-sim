{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Habitat-sim Interactivity and Advanced Features\n",
    "\n",
    "This use-case driven tutorial covers Habitat-sim interactivity and advanced features, including:\n",
    "- Adding new objects to a scene\n",
    "- Kinematic object manipulation\n",
    "- Physics simulation API\n",
    "- Agent embodiment\n",
    "- Object configuration via template libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Installation { display-mode: \"form\" }\n",
    "# @markdown (double click to show code).\n",
    "\n",
    "!curl -L https://raw.githubusercontent.com/facebookresearch/habitat-sim/master/examples/colab_utils/colab_install.sh | NIGHTLY=true bash -s\n",
    "!wget -c http://dl.fbaipublicfiles.com/habitat/mp3d_example.zip && unzip -o mp3d_example.zip -d /content/habitat-sim/data/scene_datasets/mp3d/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Path Setup and Imports { display-mode: \"form\" }\n",
    "# @markdown (double click to show code).\n",
    "\n",
    "import functools\n",
    "%cd /content/habitat-sim\n",
    "## [setup]\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import git\n",
    "import magnum as mn\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display as ipydisplay\n",
    "    # For using jupyter/ipywidget IO components\n",
    "    from ipywidgets import fixed, interact, interact_manual, interactive\n",
    "\n",
    "    HAS_WIDGETS = True\n",
    "except ImportError:\n",
    "    HAS_WIDGETS = False\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import habitat_sim\n",
    "from habitat_sim.utils import common as ut\n",
    "from habitat_sim.utils import viz_utils as vut\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/usr/bin/ffmpeg\"\n",
    "\n",
    "repo = git.Repo(\".\", search_parent_directories=True)\n",
    "dir_path = repo.working_tree_dir\n",
    "data_path = os.path.join(dir_path, \"data\")\n",
    "output_directory = \"examples/tutorials/interactivity_output/\"  # @param {type:\"string\"}\n",
    "output_path = os.path.join(dir_path, output_directory)\n",
    "if os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "# define some globals the first time we run.\n",
    "if not \"sim\" in globals():\n",
    "    global sim\n",
    "    sim = None\n",
    "    global obj_attr_mgr\n",
    "    obj_attr_mgr = None\n",
    "    global prim_attr_mgr\n",
    "    obj_attr_mgr = None\n",
    "    global scene_attr_mgr\n",
    "    scene_attr_mgr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Define Configuration Utility Functions { display-mode: \"form\" }\n",
    "# @markdown (double click to show code)\n",
    "\n",
    "# @markdown This cell defines a number of utility functions used throughout the tutorial to make simulator reconstruction easy:\n",
    "# @markdown - make_cfg\n",
    "# @markdown - make_default_settings\n",
    "# @markdown - make_simulator_from_settings\n",
    "\n",
    "\n",
    "def make_cfg(settings):\n",
    "    sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    sim_cfg.gpu_device_id = 0\n",
    "    sim_cfg.scene.id = settings[\"scene\"]\n",
    "    sim_cfg.enable_physics = settings[\"enable_physics\"]\n",
    "\n",
    "    # Note: all sensors must have the same resolution\n",
    "    sensors = {\n",
    "        \"color_sensor_1st_person\": {\n",
    "            \"sensor_type\": habitat_sim.SensorType.COLOR,\n",
    "            \"resolution\": [settings[\"height\"], settings[\"width\"]],\n",
    "            \"position\": [0.0, settings[\"sensor_height\"], 0.0],\n",
    "            \"orientation\": [settings[\"sensor_pitch\"], 0.0, 0.0],\n",
    "        },\n",
    "        \"depth_sensor_1st_person\": {\n",
    "            \"sensor_type\": habitat_sim.SensorType.DEPTH,\n",
    "            \"resolution\": [settings[\"height\"], settings[\"width\"]],\n",
    "            \"position\": [0.0, settings[\"sensor_height\"], 0.0],\n",
    "            \"orientation\": [settings[\"sensor_pitch\"], 0.0, 0.0],\n",
    "        },\n",
    "        \"semantic_sensor_1st_person\": {\n",
    "            \"sensor_type\": habitat_sim.SensorType.SEMANTIC,\n",
    "            \"resolution\": [settings[\"height\"], settings[\"width\"]],\n",
    "            \"position\": [0.0, settings[\"sensor_height\"], 0.0],\n",
    "            \"orientation\": [settings[\"sensor_pitch\"], 0.0, 0.0],\n",
    "        },\n",
    "        # configure the 3rd person cam specifically:\n",
    "        \"color_sensor_3rd_person\": {\n",
    "            \"sensor_type\": habitat_sim.SensorType.COLOR,\n",
    "            \"resolution\": [settings[\"height\"], settings[\"width\"]],\n",
    "            \"position\": [0.0, settings[\"sensor_height\"] + 0.2, 0.2],\n",
    "            \"orientation\": np.array([-math.pi / 4, 0, 0]),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    sensor_specs = []\n",
    "    for sensor_uuid, sensor_params in sensors.items():\n",
    "        if settings[sensor_uuid]:\n",
    "            sensor_spec = habitat_sim.SensorSpec()\n",
    "            sensor_spec.uuid = sensor_uuid\n",
    "            sensor_spec.sensor_type = sensor_params[\"sensor_type\"]\n",
    "            sensor_spec.resolution = sensor_params[\"resolution\"]\n",
    "            sensor_spec.position = sensor_params[\"position\"]\n",
    "            sensor_spec.orientation = sensor_params[\"orientation\"]\n",
    "\n",
    "            sensor_specs.append(sensor_spec)\n",
    "\n",
    "    # Here you can specify the amount of displacement in a forward action and the turn angle\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "    agent_cfg.sensor_specifications = sensor_specs\n",
    "\n",
    "    return habitat_sim.Configuration(sim_cfg, [agent_cfg])\n",
    "\n",
    "\n",
    "def make_default_settings():\n",
    "    settings = {\n",
    "        \"width\": 720,  # Spatial resolution of the observations\n",
    "        \"height\": 540,\n",
    "        \"scene\": \"./data/scene_datasets/mp3d/17DRP5sb8fy/17DRP5sb8fy.glb\",  # Scene path\n",
    "        \"default_agent\": 0,\n",
    "        \"sensor_height\": 1.5,  # Height of sensors in meters\n",
    "        \"sensor_pitch\": -math.pi / 8.0,  # sensor pitch (x rotation in rads)\n",
    "        \"color_sensor_1st_person\": True,  # RGB sensor\n",
    "        \"color_sensor_3rd_person\": False,  # RGB sensor 3rd person\n",
    "        \"depth_sensor_1st_person\": False,  # Depth sensor\n",
    "        \"semantic_sensor_1st_person\": False,  # Semantic sensor\n",
    "        \"seed\": 1,\n",
    "        \"enable_physics\": True,  # enable dynamics simulation\n",
    "    }\n",
    "    return settings\n",
    "\n",
    "\n",
    "def make_simulator_from_settings(sim_settings):\n",
    "    cfg = make_cfg(sim_settings)\n",
    "    # clean-up the current simulator instance if it exists\n",
    "    global sim\n",
    "    global obj_attr_mgr\n",
    "    global prim_attr_mgr\n",
    "    global scene_attr_mgr\n",
    "    if sim != None:\n",
    "        sim.close()\n",
    "    # initialize the simulator\n",
    "    sim = habitat_sim.Simulator(cfg)\n",
    "    # Managers of various Attributes templates\n",
    "    obj_attr_mgr = sim.get_object_template_manager()\n",
    "    prim_attr_mgr = sim.get_asset_template_manager()\n",
    "    scene_attr_mgr = sim.get_scene_template_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Simulation Utility Functions { display-mode: \"form\" }\n",
    "# @markdown (double click to show code)\n",
    "\n",
    "# @markdown - remove_all_objects\n",
    "# @markdown - simulate\n",
    "# @markdown - sample_object_state\n",
    "\n",
    "\n",
    "def remove_all_objects(sim):\n",
    "    for id in sim.get_existing_object_ids():\n",
    "        sim.remove_object(id)\n",
    "\n",
    "\n",
    "def simulate(sim, dt=1.0, get_frames=True):\n",
    "    # simulate dt seconds at 60Hz to the nearest fixed timestep\n",
    "    print(\"Simulating \" + str(dt) + \" world seconds.\")\n",
    "    observations = []\n",
    "    start_time = sim.get_world_time()\n",
    "    while sim.get_world_time() < start_time + dt:\n",
    "        sim.step_physics(1.0 / 60.0)\n",
    "        if get_frames:\n",
    "            observations.append(sim.get_sensor_observations())\n",
    "    return observations\n",
    "\n",
    "\n",
    "# Set an object transform relative to the agent state\n",
    "def set_object_state_from_agent(\n",
    "    sim,\n",
    "    ob_id,\n",
    "    offset=np.array([0, 2.0, -1.5]),\n",
    "    orientation=mn.Quaternion(((0, 0, 0), 1)),\n",
    "):\n",
    "    agent_transform = sim.agents[0].scene_node.transformation_matrix()\n",
    "    ob_translation = agent_transform.transform_point(offset)\n",
    "    sim.set_translation(ob_translation, ob_id)\n",
    "    sim.set_rotation(orientation, ob_id)\n",
    "\n",
    "\n",
    "# sample a random valid state for the object from the scene bounding box or navmesh\n",
    "def sample_object_state(\n",
    "    sim, object_id, from_navmesh, maintain_object_up=True, max_tries=100, bb=None\n",
    "):\n",
    "    # check that the object is not STATIC\n",
    "    if sim.get_object_motion_type(object_id) is habitat_sim.physics.MotionType.STATIC:\n",
    "        print(\"sample_object_state : Object is STATIC, aborting.\")\n",
    "    if from_navmesh:\n",
    "        if not sim.pathfinder.is_loaded:\n",
    "            print(\"sample_object_state : No pathfinder, aborting.\")\n",
    "            return False\n",
    "    elif not bb:\n",
    "        print(\n",
    "            \"sample_object_state : from_navmesh not specified and no bounding box provided, aborting.\"\n",
    "        )\n",
    "        return False\n",
    "    tries = 0\n",
    "    valid_placement = False\n",
    "    # get scene bounding box\n",
    "    # scene_bb = sim.get_active_scene_graph().get_root_node().cumulative_bb\n",
    "    while not valid_placement and tries < max_tries:\n",
    "        tries += 1\n",
    "        # initialize sample location to random point in scene bounding box\n",
    "        sample_location = np.array([0, 0, 0])\n",
    "        if from_navmesh:\n",
    "            # query random navigable point\n",
    "            sample_location = sim.pathfinder.get_random_navigable_point()\n",
    "        else:\n",
    "            sample_location = np.random.uniform(bb.min, bb.max)\n",
    "        # set the test state\n",
    "        sim.set_translation(sample_location, object_id)\n",
    "        if maintain_object_up:\n",
    "            # random rotation only on the Y axis\n",
    "            y_rotation = mn.Quaternion.rotation(\n",
    "                mn.Rad(random.random() * 2 * math.pi), mn.Vector3(0, 1.0, 0)\n",
    "            )\n",
    "            sim.set_rotation(y_rotation * sim.get_rotation(object_id), object_id)\n",
    "        else:\n",
    "            # unconstrained random rotation\n",
    "            sim.set_rotation(ut.random_quaternion(), object_id)\n",
    "\n",
    "        # raise object such that lowest bounding box corner is above the navmesh sample point.\n",
    "        if from_navmesh:\n",
    "            obj_node = sim.get_object_scene_node(object_id)\n",
    "            xform_bb = habitat_sim.geo.get_transformed_bb(\n",
    "                obj_node.cumulative_bb, obj_node.transformation\n",
    "            )\n",
    "            # also account for collision margin of the scene\n",
    "            scene_collision_margin = 0.04\n",
    "            y_translation = mn.Vector3(\n",
    "                0, xform_bb.size_y() / 2.0 + scene_collision_margin, 0\n",
    "            )\n",
    "            sim.set_translation(\n",
    "                y_translation + sim.get_translation(object_id), object_id\n",
    "            )\n",
    "\n",
    "        # test for penetration with the environment\n",
    "        if not sim.contact_test(object_id):\n",
    "            valid_placement = True\n",
    "\n",
    "    if not valid_placement:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Template Dictionary Utility Functions { display-mode: \"form\" }\n",
    "# @markdown (double click to show code)\n",
    "\n",
    "# @markdown This cell defines utility functions to easily peak into Attribute template contents.\n",
    "\n",
    "# This method builds a dictionary of k-v pairs of attribute property names and\n",
    "# values shared by all attribute template types.  The values are tuples with the\n",
    "# first entry being the value and the second being whether the property is\n",
    "# editable and the third being the type.\n",
    "def build_dict_of_Default_attrs(template):\n",
    "    res_dict = {}\n",
    "    res_dict[\"handle\"] = (template.handle, True, \"string\")\n",
    "    # Read-only values\n",
    "    res_dict[\"ID\"] = (template.ID, False, \"int\")\n",
    "    res_dict[\"template_class\"] = (template.template_class, False, \"string\")\n",
    "    # New fields, uncomment upon updating conda 8/4/20\n",
    "    res_dict[\"file_directory\"] = (template.file_directory, False, \"string\")\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "# This method builds a dictionary of k-v pairs of attribute property names and\n",
    "# values shared by templates of physically modeled constructs (scenes and\n",
    "# objects). The values are tuples with the first entry being the value and the\n",
    "# second being whether the property is editable and the third being the type.\n",
    "def build_dict_of_PhyObj_attrs(phys_obj_template):\n",
    "    res_dict = build_dict_of_Default_attrs(phys_obj_template)\n",
    "    res_dict[\"scale\"] = (phys_obj_template.scale, True, \"vector\")\n",
    "    res_dict[\"margin\"] = (phys_obj_template.margin, True, \"double\")\n",
    "    res_dict[\"friction_coefficient\"] = (\n",
    "        phys_obj_template.friction_coefficient,\n",
    "        True,\n",
    "        \"double\",\n",
    "    )\n",
    "    res_dict[\"restitution_coefficient\"] = (\n",
    "        phys_obj_template.restitution_coefficient,\n",
    "        True,\n",
    "        \"double\",\n",
    "    )\n",
    "    res_dict[\"render_asset_handle\"] = (\n",
    "        phys_obj_template.render_asset_handle,\n",
    "        True,\n",
    "        \"string\",\n",
    "    )\n",
    "    res_dict[\"collision_asset_handle\"] = (\n",
    "        phys_obj_template.collision_asset_handle,\n",
    "        True,\n",
    "        \"string\",\n",
    "    )\n",
    "    res_dict[\"requires_lighting\"] = (\n",
    "        phys_obj_template.requires_lighting,\n",
    "        True,\n",
    "        \"boolean\",\n",
    "    )\n",
    "    # New fields, uncomment upon updating conda 8/4/20\n",
    "    res_dict[\"orient_up\"] = (phys_obj_template.orient_up, True, \"vector\")\n",
    "    res_dict[\"orient_front\"] = (phys_obj_template.orient_front, True, \"vector\")\n",
    "    res_dict[\"units_to_meters\"] = (phys_obj_template.units_to_meters, True, \"double\")\n",
    "    res_dict[\"render_asset_type\"] = (phys_obj_template.render_asset_type, True, \"int\")\n",
    "    res_dict[\"collision_asset_type\"] = (\n",
    "        phys_obj_template.collision_asset_type,\n",
    "        True,\n",
    "        \"int\",\n",
    "    )\n",
    "\n",
    "    # Read-only values\n",
    "    res_dict[\"render_asset_is_primitive\"] = (\n",
    "        phys_obj_template.render_asset_is_primitive,\n",
    "        False,\n",
    "        \"boolean\",\n",
    "    )\n",
    "    res_dict[\"collision_asset_is_primitive\"] = (\n",
    "        phys_obj_template.collision_asset_is_primitive,\n",
    "        False,\n",
    "        \"boolean\",\n",
    "    )\n",
    "    res_dict[\"use_mesh_for_collision\"] = (\n",
    "        phys_obj_template.use_mesh_for_collision,\n",
    "        False,\n",
    "        \"boolean\",\n",
    "    )\n",
    "    res_dict[\"is_dirty\"] = (phys_obj_template.is_dirty, False, \"boolean\")\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "# This method will build a dict containing k-v pairs of attribute property names\n",
    "# and values for the passed object template. The values are tuples with the first\n",
    "# entry being the value,the second being whether the property is editable and\n",
    "# the third being the type.\n",
    "def build_dict_of_Object_attrs(obj_template):\n",
    "    res_dict = build_dict_of_PhyObj_attrs(obj_template)\n",
    "    res_dict[\"com\"] = (obj_template.com, True, \"vector\")\n",
    "    res_dict[\"compute_COM_from_shape\"] = (\n",
    "        obj_template.compute_COM_from_shape,\n",
    "        True,\n",
    "        \"boolean\",\n",
    "    )\n",
    "    res_dict[\"mass\"] = (obj_template.mass, True, \"double\")\n",
    "    res_dict[\"inertia\"] = (obj_template.inertia, True, \"vector\")\n",
    "    res_dict[\"linear_damping\"] = (obj_template.linear_damping, True, \"double\")\n",
    "    res_dict[\"angular_damping\"] = (obj_template.angular_damping, True, \"double\")\n",
    "    res_dict[\"bounding_box_collisions\"] = (\n",
    "        obj_template.bounding_box_collisions,\n",
    "        True,\n",
    "        \"boolean\",\n",
    "    )\n",
    "    res_dict[\"join_collision_meshes\"] = (\n",
    "        obj_template.join_collision_meshes,\n",
    "        True,\n",
    "        \"boolean\",\n",
    "    )\n",
    "    # New fields, uncomment upon updating conda 8/4/20\n",
    "    # res_dict[\"is_visible\"] = (obj_template.is_visible, True, \"boolean\")\n",
    "    # res_dict[\"is_collidable\"] = (obj_template.is_collidable, True, \"boolean\")\n",
    "    res_dict[\"semantic_id\"] = (obj_template.semantic_id, True, \"int\")\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "# This method will build a dict containing k-v pairs of attribute property names\n",
    "# and values for the passed scene template. The values are tuples with the first\n",
    "# entry being the value,the second being whether the property is editable and\n",
    "# the third being the type.\n",
    "def build_dict_of_Scene_attrs(scene_template):\n",
    "    res_dict = build_dict_of_PhyObj_attrs(scene_template)\n",
    "    res_dict[\"gravity\"] = (scene_template.gravity, True, \"vector\")\n",
    "    res_dict[\"origin\"] = (scene_template.origin, True, \"vector\")\n",
    "    # New fields, uncomment upon updating conda 8/4/20\n",
    "    res_dict[\"semantic_asset_handle\"] = (\n",
    "        scene_template.semantic_asset_handle,\n",
    "        True,\n",
    "        \"string\",\n",
    "    )\n",
    "    res_dict[\"semantic_asset_type\"] = (scene_template.semantic_asset_type, True, \"int\")\n",
    "    res_dict[\"navmesh_asset_handle\"] = (\n",
    "        scene_template.navmesh_asset_handle,\n",
    "        True,\n",
    "        \"string\",\n",
    "    )\n",
    "    res_dict[\"house_filename\"] = (scene_template.house_filename, True, \"string\")\n",
    "    # res_dict[\"light_setup\"] = (scene_template.light_setup, True, \"string\")\n",
    "    # res_dict[\"frustrum_culling\"] = (scene_template.frustrum_culling, True, \"boolean\")\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "# This method will build a dict containing k-v pairs of attribute property names\n",
    "# and values for the passed physics manager template. The values are tuples with\n",
    "# the first entry being the value,the second being whether the property is\n",
    "# editable and the third being the type.\n",
    "def build_dict_of_PhysicsSim_attrs(physics_template):\n",
    "    res_dict = build_dict_of_Default_attrs(physics_template)\n",
    "    res_dict[\"gravity\"] = (physics_template.gravity, True, \"vector\")\n",
    "    res_dict[\"timestep\"] = (physics_template.timestep, True, \"double\")\n",
    "    res_dict[\"max_substeps\"] = (physics_template.max_substeps, True, \"int\")\n",
    "    res_dict[\"friction_coefficient\"] = (\n",
    "        physics_template.friction_coefficient,\n",
    "        True,\n",
    "        \"double\",\n",
    "    )\n",
    "    res_dict[\"restitution_coefficient\"] = (\n",
    "        physics_template.restitution_coefficient,\n",
    "        True,\n",
    "        \"double\",\n",
    "    )\n",
    "\n",
    "    # Read-only values\n",
    "    res_dict[\"simulator\"] = (physics_template.simulator, False, \"string\")\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "# This method will build a dict containing k-v pairs of attribute property names\n",
    "# and values that are shared among all primitive asset attributes templates.\n",
    "# The values are tuples with the first entry being the value,the second being\n",
    "# whether the property is editable and the third being the type.\n",
    "def build_dict_of_prim_attrs(prim_template):\n",
    "    res_dict = build_dict_of_Default_attrs(prim_template)\n",
    "    res_dict[\"use_texture_coords\"] = (prim_template.use_texture_coords, True, \"boolean\")\n",
    "    res_dict[\"use_tangents\"] = (prim_template.use_tangents, True, \"boolean\")\n",
    "    res_dict[\"num_rings\"] = (prim_template.num_rings, True, \"int\")\n",
    "    res_dict[\"num_segments\"] = (prim_template.num_segments, True, \"int\")\n",
    "    res_dict[\"half_length\"] = (prim_template.half_length, True)\n",
    "\n",
    "    # Read-only values\n",
    "    res_dict[\"prim_obj_class_name\"] = (\n",
    "        prim_template.prim_obj_class_name,\n",
    "        False,\n",
    "        \"string\",\n",
    "    )\n",
    "    res_dict[\"prim_obj_type\"] = (prim_template.prim_obj_type, False, \"int\")\n",
    "    res_dict[\"is_valid_template\"] = (prim_template.is_valid_template, False, \"boolean\")\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "# This method will build a dict containing k-v pairs of attribute property names\n",
    "# and values for the passed capsule primitive template. The values are tuples with\n",
    "# the first entry being the value,the second being whether the property is\n",
    "# editable and the third being the type.\n",
    "def build_dict_of_Capsule_prim_attrs(capsule_template):\n",
    "    res_dict = build_dict_of_prim_attrs(capsule_template)\n",
    "    res_dict[\"hemisphere_rings\"] = (capsule_template.hemisphere_rings, True, \"int\")\n",
    "    res_dict[\"cylinder_rings\"] = (capsule_template.cylinder_rings, True, \"int\")\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "# This method will build a dict containing k-v pairs of attribute property names\n",
    "# and values for the passed cone primitive template. The values are tuples with\n",
    "# the first entry being the value,the second being whether the property is\n",
    "# editable and the third being the type.\n",
    "def build_dict_of_Cone_prim_attrs(cone_template):\n",
    "    res_dict = build_dict_of_prim_attrs(cone_template)\n",
    "    res_dict[\"use_cap_end\"] = (cone_template.use_cap_end, True, \"boolean\")\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "# This method will build a dict containing k-v pairs of attribute property names\n",
    "# and values for the passed cube primitive template. The values are tuples with\n",
    "# the first entry being the value,the second being whether the property is\n",
    "# editable and the third being the type.\n",
    "def build_dict_of_Cube_prim_attrs(cube_template):\n",
    "    res_dict = build_dict_of_prim_attrs(cube_template)\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "# This method will build a dict containing k-v pairs of attribute property names\n",
    "# and values for the passed cylinder primitive template. The values are tuples with\n",
    "# the first entry being the value,the second being whether the property is\n",
    "# editable and the third being the type.\n",
    "def build_dict_of_Cylinder_prim_attrs(cylinder_template):\n",
    "    res_dict = build_dict_of_prim_attrs(cylinder_template)\n",
    "    res_dict[\"use_cap_ends\"] = (cylinder_template.use_cap_ends, True, \"boolean\")\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "# This method will build a dict containing k-v pairs of attribute property names\n",
    "# and values for the passed icosphere primitive template. The values are tuples with\n",
    "# the first entry being the value,the second being whether the property is\n",
    "# editable and the third being the type.\n",
    "def build_dict_of_Icosphere_prim_attrs(icosphere_template):\n",
    "    res_dict = build_dict_of_prim_attrs(icosphere_template)\n",
    "    res_dict[\"subdivisions\"] = (icosphere_template.subdivisions, True, \"int\")\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "# This method will build a dict containing k-v pairs of attribute property names\n",
    "# and values for the passed UV-Sphere primitive template. The values are tuples with\n",
    "# the first entry being the value,the second being whether the property is\n",
    "# editable and the third being the type.\n",
    "def build_dict_of_UVSphere_prim_attrs(uvsphere_template):\n",
    "    res_dict = build_dict_of_prim_attrs(uvsphere_template)\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "# This method will deduce the appropriate template type and build the subsequent\n",
    "# dictionary containing containing k-v pairs of attribute property names\n",
    "# and values for the passed template. The values are tuples with\n",
    "# the first entry being the value,the second being whether the property is\n",
    "# editable and the third being the type.\n",
    "def build_dict_of_attrs(template):\n",
    "    template_class = template.template_class\n",
    "    if \"PhysicsObjectAttributes\" in template_class:\n",
    "        return build_dict_of_Object_attrs(template)\n",
    "    if \"PhysicsSceneAttributes\" in template_class:\n",
    "        return build_dict_of_Scene_attrs(template)\n",
    "    if \"PhysicsManagerAttributes\" in template_class:\n",
    "        return build_dict_of_PhysicsSim_attrs(template)\n",
    "    if \"CapsulePrimitiveAttributes\" in template_class:\n",
    "        return build_dict_of_Capsule_prim_attrs(template)\n",
    "    if \"ConePrimitiveAttributes\" in template_class:\n",
    "        return build_dict_of_Cone_prim_attrs(template)\n",
    "    if \"CubePrimitiveAttributes\" in template_class:\n",
    "        return build_dict_of_Cube_prim_attrs(template)\n",
    "    if \"CylinderPrimitiveAttributes\" in template_class:\n",
    "        return build_dict_of_Cylinder_prim_attrs(template)\n",
    "    if \"IcospherePrimitiveAttributes\" in template_class:\n",
    "        return build_dict_of_Icosphere_prim_attrs(template)\n",
    "    if \"UVSpherePrimitiveAttributes\" in template_class:\n",
    "        return build_dict_of_UVSphere_prim_attrs(template)\n",
    "    print(\"Unknown template type : %s \" % template_class)\n",
    "    return None\n",
    "\n",
    "\n",
    "# This will set a template's attributes from the passed map\n",
    "def set_template_vals_from_map(template, template_map):\n",
    "    for k, v in template_map.items():\n",
    "        setattr(template, k, v[0])\n",
    "    return template\n",
    "\n",
    "\n",
    "# This will display all the known values of an attributes template\n",
    "def show_template_map_vals(template):\n",
    "    template_map = build_dict_of_attrs(template)\n",
    "    print(\"Template {} has : \".format(template.handle))\n",
    "    for k, v in template_map.items():\n",
    "        print(\n",
    "            \"\\t key {} with value {} of type {} that is editable : {}\".format(\n",
    "                k, v[0], v[2], v[1]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Visualization Utility Functions { display-mode: \"form\" }\n",
    "# @markdown (double click to show code)\n",
    "# @markdown - make_video_cv2\n",
    "# @markdown - display_sample\n",
    "def make_video_cv2(observations, prefix=\"\", open_vid=True, multi_obs=False, fps=60):\n",
    "    sensor_keys = list(observations[0])\n",
    "    videodims = observations[0][sensor_keys[0]].shape\n",
    "    videodims = (videodims[1], videodims[0])  # flip to w,h order\n",
    "    print(videodims)\n",
    "    video_file = output_path + prefix + \".mp4\"\n",
    "    print(\"Encoding the video: %s \" % video_file)\n",
    "    writer = vut.get_fast_video_writer(video_file, fps=fps)\n",
    "    thumb_size = (int(videodims[0] / 5), int(videodims[1] / 5))\n",
    "    outline_frame = np.ones((thumb_size[1] + 2, thumb_size[0] + 2, 3), np.uint8) * 150\n",
    "    for ob in observations:\n",
    "\n",
    "        # If in RGB/RGBA format, remove the alpha channel\n",
    "        rgb_im_1st_person = cv2.cvtColor(\n",
    "            ob[\"color_sensor_1st_person\"], cv2.COLOR_RGBA2RGB\n",
    "        )\n",
    "\n",
    "        if multi_obs:\n",
    "            # embed the 1st person RBG frame into the 3rd person frame\n",
    "            rgb_im_3rd_person = cv2.cvtColor(\n",
    "                ob[\"color_sensor_3rd_person\"], cv2.COLOR_RGBA2RGB\n",
    "            )\n",
    "            resized_1st_person_rgb = cv2.resize(\n",
    "                rgb_im_1st_person, thumb_size, interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "            x_offset = 50\n",
    "            y_offset_rgb = 50\n",
    "            rgb_im_3rd_person[\n",
    "                y_offset_rgb - 1 : y_offset_rgb + outline_frame.shape[0] - 1,\n",
    "                x_offset - 1 : x_offset + outline_frame.shape[1] - 1,\n",
    "            ] = outline_frame\n",
    "            rgb_im_3rd_person[\n",
    "                y_offset_rgb : y_offset_rgb + resized_1st_person_rgb.shape[0],\n",
    "                x_offset : x_offset + resized_1st_person_rgb.shape[1],\n",
    "            ] = resized_1st_person_rgb\n",
    "\n",
    "            # embed the 1st person DEPTH frame into the 3rd person frame\n",
    "            # manually normalize depth into [0, 1] so that images are always consistent\n",
    "            d_im = np.clip(ob[\"depth_sensor_1st_person\"], 0, 10)\n",
    "            d_im /= 10.0\n",
    "            bgr_d_im = cv2.cvtColor((d_im * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "            resized_1st_person_depth = cv2.resize(\n",
    "                bgr_d_im, thumb_size, interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "            y_offset_d = y_offset_rgb + 10 + thumb_size[1]\n",
    "            rgb_im_3rd_person[\n",
    "                y_offset_d - 1 : y_offset_d + outline_frame.shape[0] - 1,\n",
    "                x_offset - 1 : x_offset + outline_frame.shape[1] - 1,\n",
    "            ] = outline_frame\n",
    "            rgb_im_3rd_person[\n",
    "                y_offset_d : y_offset_d + resized_1st_person_depth.shape[0],\n",
    "                x_offset : x_offset + resized_1st_person_depth.shape[1],\n",
    "            ] = resized_1st_person_depth\n",
    "            if rgb_im_3rd_person.shape[:2] != videodims:\n",
    "                rgb_im_3rd_person = cv2.resize(\n",
    "                    rgb_im_3rd_person, videodims, interpolation=cv2.INTER_AREA\n",
    "                )\n",
    "            # write the video frame\n",
    "            writer.append_data(rgb_im_3rd_person)\n",
    "        else:\n",
    "            if rgb_im_1st_person.shape[:2] != videodims:\n",
    "                rgb_im_1st_person = cv2.resize(\n",
    "                    rgb_im_1st_person, videodims, interpolation=cv2.INTER_AREA\n",
    "                )\n",
    "            # write the 1st person observation to video\n",
    "            writer.append_data(rgb_im_1st_person)\n",
    "    writer.close()\n",
    "\n",
    "    if open_vid:\n",
    "        print(\"Displaying video\")\n",
    "        vut.display_video(video_file)\n",
    "\n",
    "\n",
    "# Change to do something like this maybe: https://stackoverflow.com/a/41432704\n",
    "def display_sample(\n",
    "    rgb_obs, semantic_obs=np.array([]), depth_obs=np.array([]), key_points=None\n",
    "):\n",
    "    from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGBA\")\n",
    "\n",
    "    arr = [rgb_img]\n",
    "    titles = [\"rgb\"]\n",
    "    if semantic_obs.size != 0:\n",
    "        semantic_img = Image.new(\"P\", (semantic_obs.shape[1], semantic_obs.shape[0]))\n",
    "        semantic_img.putpalette(d3_40_colors_rgb.flatten())\n",
    "        semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "        semantic_img = semantic_img.convert(\"RGBA\")\n",
    "        arr.append(semantic_img)\n",
    "        titles.append(\"semantic\")\n",
    "\n",
    "    if depth_obs.size != 0:\n",
    "        depth_img = Image.fromarray((depth_obs / 10 * 255).astype(np.uint8), mode=\"L\")\n",
    "        arr.append(depth_img)\n",
    "        titles.append(\"depth\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, data in enumerate(arr):\n",
    "        ax = plt.subplot(1, 3, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(titles[i])\n",
    "        # plot points on images\n",
    "        if key_points is not None:\n",
    "            for pix, point in enumerate(key_points):\n",
    "                plt.plot(point[0], point[1], marker=\"o\", markersize=10, alpha=0.8)\n",
    "        plt.imshow(data)\n",
    "\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--no-display\", dest=\"display\", action=\"store_false\")\n",
    "    parser.add_argument(\"--no-make-video\", dest=\"make_video\", action=\"store_false\")\n",
    "    parser.set_defaults(show_video=True, make_video=True)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    show_video = args.display\n",
    "    display = args.display\n",
    "    make_video = args.make_video\n",
    "else:\n",
    "    show_video = False\n",
    "    make_video = False\n",
    "    display = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Colab GUI Utility Functions { display-mode: \"form\" }\n",
    "# @markdown (double click to show code)\n",
    "\n",
    "# Event handler for dropdowns displaying file-based object handles\n",
    "def on_file_obj_ddl_change(ddl_values):\n",
    "    global sel_file_obj_handle\n",
    "    sel_file_obj_handle = ddl_values[\"new\"]\n",
    "    return sel_file_obj_handle\n",
    "\n",
    "\n",
    "# Event handler for dropdowns displaying prim-based object handles\n",
    "def on_prim_obj_ddl_change(ddl_values):\n",
    "    global sel_prim_obj_handle\n",
    "    sel_prim_obj_handle = ddl_values[\"new\"]\n",
    "    return sel_prim_obj_handle\n",
    "\n",
    "\n",
    "# Event handler for dropdowns displaying asset handles\n",
    "def on_prim_ddl_change(ddl_values):\n",
    "    global sel_asset_handle\n",
    "    sel_asset_handle = ddl_values[\"new\"]\n",
    "    return sel_asset_handle\n",
    "\n",
    "\n",
    "# Build a dropdown list holding obj_handles and set its event handler\n",
    "def set_handle_ddl_widget(obj_handles, handle_types, sel_handle, on_change):\n",
    "    sel_handle = obj_handles[0]\n",
    "    descStr = handle_types + \" Template Handles:\"\n",
    "    style = {\"description_width\": \"300px\"}\n",
    "    obj_ddl = widgets.Dropdown(\n",
    "        options=obj_handles,\n",
    "        value=sel_handle,\n",
    "        description=descStr,\n",
    "        style=style,\n",
    "        disabled=False,\n",
    "        layout={\"width\": \"max-content\"},\n",
    "    )\n",
    "\n",
    "    obj_ddl.observe(on_change, names=\"value\")\n",
    "    return obj_ddl, sel_handle\n",
    "\n",
    "\n",
    "def set_button_launcher(desc):\n",
    "    button = widgets.Button(description=desc, layout={\"width\": \"max-content\"},)\n",
    "    return button\n",
    "\n",
    "\n",
    "def make_sim_and_vid_button(prefix, dt=1.0):\n",
    "    if not HAS_WIDGETS:\n",
    "        return\n",
    "\n",
    "    def on_sim_click(b):\n",
    "        observations = simulate(sim, dt=dt)\n",
    "        make_video_cv2(observations, prefix=prefix, open_vid=True, multi_obs=False)\n",
    "\n",
    "    sim_and_vid_btn = set_button_launcher(\"Simulate and Make Video\")\n",
    "    sim_and_vid_btn.on_click(on_sim_click)\n",
    "    ipydisplay(sim_and_vid_btn)\n",
    "\n",
    "\n",
    "def make_clear_all_objects_button():\n",
    "    if not HAS_WIDGETS:\n",
    "        return\n",
    "\n",
    "    def on_clear_click(b):\n",
    "        remove_all_objects(sim)\n",
    "\n",
    "    clear_objs_button = set_button_launcher(\"Clear all objects\")\n",
    "    clear_objs_button.on_click(on_clear_click)\n",
    "    ipydisplay(clear_objs_button)\n",
    "\n",
    "\n",
    "# Builds widget-based UI components\n",
    "def build_widget_ui(obj_attr_mgr, prim_attr_mgr):\n",
    "    # Holds the user's desired file-based object template handle\n",
    "    global sel_file_obj_handle\n",
    "    sel_file_obj_handle = \"\"\n",
    "\n",
    "    # Holds the user's desired primitive-based object template handle\n",
    "    global sel_prim_obj_handle\n",
    "    sel_prim_obj_handle = \"\"\n",
    "\n",
    "    # Holds the user's desired primitive asset template handle\n",
    "    global sel_asset_handle\n",
    "    sel_asset_handle = \"\"\n",
    "\n",
    "    # Construct DDLs and assign event handlers\n",
    "    # All file-based object template handles\n",
    "    file_obj_handles = obj_attr_mgr.get_file_template_handles()\n",
    "    # All primitive asset-based object template handles\n",
    "    prim_obj_handles = obj_attr_mgr.get_synth_template_handles()\n",
    "    # All primitive asset handles template handles\n",
    "    prim_asset_handles = prim_attr_mgr.get_template_handles()\n",
    "    if not HAS_WIDGETS:\n",
    "        sel_file_obj_handle = file_obj_handles[0]\n",
    "        sel_prim_obj_handle = prim_obj_handles[0]\n",
    "        sel_prim_obj_handle = prim_asset_handles[0]\n",
    "        return\n",
    "    file_obj_ddl, sel_file_obj_handle = set_handle_ddl_widget(\n",
    "        file_obj_handles,\n",
    "        \"File-based Object\",\n",
    "        sel_file_obj_handle,\n",
    "        on_file_obj_ddl_change,\n",
    "    )\n",
    "    prim_obj_ddl, sel_prim_obj_handle = set_handle_ddl_widget(\n",
    "        prim_obj_handles,\n",
    "        \"Primitive-based Object\",\n",
    "        sel_prim_obj_handle,\n",
    "        on_prim_obj_ddl_change,\n",
    "    )\n",
    "    prim_asset_ddl, sel_asset_handle = set_handle_ddl_widget(\n",
    "        prim_asset_handles, \"Primitive Asset\", sel_asset_handle, on_prim_ddl_change\n",
    "    )\n",
    "    # Display DDLs\n",
    "    ipydisplay(file_obj_ddl)\n",
    "    ipydisplay(prim_obj_ddl)\n",
    "    ipydisplay(prim_asset_ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initialize Simulator and Load Scene { display-mode: \"form\" }\n",
    "\n",
    "# convienience functions defined in Utility cell manage global variables\n",
    "sim_settings = make_default_settings()\n",
    "# set globals: sim,\n",
    "make_simulator_from_settings(sim_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Interactivity in Habitat-sim\n",
    "\n",
    "This tutorial section covers how to configure and use the Habitat-sim object manipulation API to setup and run physical interaction simulations.\n",
    "\n",
    "## Outline:\n",
    "This section is divided into three use-case driven sub-sections:\n",
    "1.   Introduction to Interactivity\n",
    "2.   Physical Reasoning\n",
    "3.   Continuous Embodied Navigation\n",
    "\n",
    "For more tutorial examples and details see the [Interactive Rigid Objects tutorial](https://aihabitat.org/docs/habitat-sim/rigid-object-tutorial.html) also available for Colab [here](https://github.com/facebookresearch/habitat-sim/blob/master/examples/tutorials/colabs/rigid_object_tutorial.ipynb).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Interactivity\n",
    "\n",
    "####Easily add an object and simulate!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Select a Simulation Object: { display-mode: \"form\" }\n",
    "# @markdown Use the dropdown menu below to select an object for use in the\n",
    "# @markdown following examples.\n",
    "\n",
    "# @markdown File-based objects are loaded from and named after an asset file (e.g. banana.glb).\n",
    "\n",
    "# @markdown Primitive objects are generated programmatically (e.g. uv_sphere) with\n",
    "# @markdown handles (name/key for reference) uniquely generated from a specific parameterization.\n",
    "\n",
    "# @markdown See the [Advanced Features](https://colab.research.google.com/drive/10iSSLQZiKJi86intkenN53iOQ-w5By1z?authuser=1#scrollTo=JSrIT5nz-1Os)\n",
    "# @markdown section for more details about asset configuration.\n",
    "\n",
    "build_widget_ui(obj_attr_mgr, prim_attr_mgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Add either a File-based or Primitive Asset-based object to the scene at a user-specified location.\n",
    "# @markdown Running this will add a physically-modelled object of the selected\n",
    "# @markdown type to the scene at the location specified by user, simulate forward for a few seconds and save a\n",
    "# @markdown movie of the results.\n",
    "\n",
    "# @markdown Choose either the primitive or file-based asset recently selected in the dropdown:\n",
    "obj_template_handle = sel_file_obj_handle\n",
    "asset_tempalte_handle = sel_asset_handle\n",
    "object_type = \"File-based\"  # @param [\"File-based\",\"Primitive-based\"]\n",
    "if \"File\" in object_type:\n",
    "    # Handle File-based object handle\n",
    "    obj_template_handle = sel_file_obj_handle\n",
    "elif \"Primitive\" in object_type:\n",
    "    # Handle Primitive-based object handle\n",
    "    obj_template_handle = sel_prim_obj_handle\n",
    "else:\n",
    "    # Unknown - defaults to file-based\n",
    "    pass\n",
    "\n",
    "# @markdown Configure the initial object location (local offset from the agent body node):\n",
    "# default : offset=np.array([0,2.0,-1.5]), orientation=np.quaternion(1,0,0,0)\n",
    "offset_x = 0.5  # @param {type:\"slider\", min:-2, max:2, step:0.1}\n",
    "offset_y = 1.4  # @param {type:\"slider\", min:0, max:3.0, step:0.1}\n",
    "offset_z = -1.5  # @param {type:\"slider\", min:-3, max:0, step:0.1}\n",
    "offset = np.array([offset_x, offset_y, offset_z])\n",
    "\n",
    "# @markdown Configure the initial object orientation via local Euler angle (degrees):\n",
    "orientation_x = 0  # @param {type:\"slider\", min:-180, max:180, step:1}\n",
    "orientation_y = 0  # @param {type:\"slider\", min:-180, max:180, step:1}\n",
    "orientation_z = 0  # @param {type:\"slider\", min:-180, max:180, step:1}\n",
    "\n",
    "# compose the rotations\n",
    "rotation_x = mn.Quaternion.rotation(mn.Deg(orientation_x), mn.Vector3(1.0, 0, 0))\n",
    "rotation_y = mn.Quaternion.rotation(mn.Deg(orientation_y), mn.Vector3(1.0, 0, 0))\n",
    "rotation_z = mn.Quaternion.rotation(mn.Deg(orientation_z), mn.Vector3(1.0, 0, 0))\n",
    "orientation = rotation_z * rotation_y * rotation_x\n",
    "\n",
    "# Add object instantiated by desired template using template handle\n",
    "obj_id_1 = sim.add_object_by_handle(obj_template_handle)\n",
    "\n",
    "# @markdown Note: agent local coordinate system is Y up and -Z forward.\n",
    "# Move object to be in front of the agent\n",
    "set_object_state_from_agent(sim, obj_id_1, offset=offset, orientation=orientation)\n",
    "\n",
    "# display a still frame of the scene after the object is added if RGB sensor is enabled\n",
    "observations = sim.get_sensor_observations()\n",
    "if display:\n",
    "    if sim_settings[\"color_sensor_1st_person\"]:\n",
    "        display_sample(observations[\"color_sensor_1st_person\"])\n",
    "\n",
    "example_type = \"adding objects test\"\n",
    "make_sim_and_vid_button(example_type)\n",
    "make_clear_all_objects_button()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Physical Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section demonstrates simple setups for physical reasoning tasks in Habitat-sim with a fixed camera position collecting data:\n",
    "- [Scripted vs. Dynamic Motion](https://colab.research.google.com/drive/10iSSLQZiKJi86intkenN53iOQ-w5By1z?authuser=1#scrollTo=oUlgE5P-_F65&line=1&uniqifier=1)\n",
    "- [Object Permanence](https://colab.research.google.com/drive/10iSSLQZiKJi86intkenN53iOQ-w5By1z?authuser=1#scrollTo=GTPL4fzY_GZt&line=1&uniqifier=1)\n",
    "- [Physical plausibility classification](https://colab.research.google.com/drive/10iSSLQZiKJi86intkenN53iOQ-w5By1z?authuser=1#scrollTo=DH3mLjq5PabM&line=1&uniqifier=1)\n",
    "- [Trajectory Prediction]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Select objects from the GUI: { display-mode: \"form\" }\n",
    "\n",
    "build_widget_ui(obj_attr_mgr, prim_attr_mgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Scripted vs. Dynamic Motion\n",
    "# @markdown A quick script to generate video data for AI classification of\n",
    "# @markdown dynamically dropping vs. kinematically moving objects.\n",
    "remove_all_objects(sim)\n",
    "# @markdown Set the scene as dynamic or kinematic:\n",
    "scenario_is_kinematic = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# add the selected object\n",
    "obj_id_1 = sim.add_object_by_handle(sel_file_obj_handle)\n",
    "\n",
    "# place the object\n",
    "set_object_state_from_agent(\n",
    "    sim, obj_id_1, offset=np.array([0, 2.0, -1.0]), orientation=ut.random_quaternion()\n",
    ")\n",
    "\n",
    "if scenario_is_kinematic:\n",
    "    # use the velocity control struct to setup a constant rate kinematic motion\n",
    "    sim.set_object_motion_type(habitat_sim.MotionType.KINEMATIC, obj_id_1)\n",
    "    vel_control = sim.get_object_velocity_control(obj_id_1)\n",
    "    vel_control.controlling_lin_vel = True\n",
    "    vel_control.linear_velocity = np.array([0, -1.0, 0])\n",
    "\n",
    "# simulate and collect observations\n",
    "example_type = \"kinematic vs dynamic\"\n",
    "observations = simulate(sim, dt=2.0)\n",
    "if make_video:\n",
    "    make_video_cv2(\n",
    "        observations, prefix=example_type, open_vid=show_video, multi_obs=False\n",
    "    )\n",
    "remove_all_objects(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Object Permanence\n",
    "# @markdown This example script demonstrates a possible object permanence task.\n",
    "# @markdown Two objects are dropped behind an occluder. One is removed while occluded.\n",
    "remove_all_objects(sim)\n",
    "\n",
    "# @markdown 1. Add the two dynamic objects.\n",
    "# add the selected objects\n",
    "obj_id_1 = sim.add_object_by_handle(sel_file_obj_handle)\n",
    "obj_id_2 = sim.add_object_by_handle(sel_file_obj_handle)\n",
    "\n",
    "# place the objects\n",
    "set_object_state_from_agent(\n",
    "    sim, obj_id_1, offset=np.array([0.5, 2.0, -1.0]), orientation=ut.random_quaternion()\n",
    ")\n",
    "set_object_state_from_agent(\n",
    "    sim,\n",
    "    obj_id_2,\n",
    "    offset=np.array([-0.5, 2.0, -1.0]),\n",
    "    orientation=ut.random_quaternion(),\n",
    ")\n",
    "\n",
    "# @markdown 2. Configure and add an occluder from a scaled cube primitive.\n",
    "# Get a default cube primitive template\n",
    "obj_attr_mgr = sim.get_object_template_manager()\n",
    "cube_handle = obj_attr_mgr.get_template_handles(\"cube\")[0]\n",
    "cube_template_cpy = obj_attr_mgr.get_template_by_handle(cube_handle)\n",
    "# Modify the template's configured scale.\n",
    "cube_template_cpy.scale = np.array([0.32, 0.075, 0.01])\n",
    "# Register the modified template under a new name.\n",
    "obj_attr_mgr.register_template(cube_template_cpy, \"occluder_cube\")\n",
    "# Instance and place the occluder object from the template.\n",
    "occluder_id = sim.add_object_by_handle(\"occluder_cube\")\n",
    "set_object_state_from_agent(sim, occluder_id, offset=np.array([0.0, 1.4, -0.4]))\n",
    "sim.set_object_motion_type(habitat_sim.MotionType.KINEMATIC, occluder_id)\n",
    "\n",
    "# @markdown 3. Simulate at 60Hz, removing one object when it's center of mass\n",
    "# @markdown drops below that of the occluder.\n",
    "# Simulate and remove object when it passes the midpoint of the occluder\n",
    "dt = 2.0\n",
    "print(\"Simulating \" + str(dt) + \" world seconds.\")\n",
    "observations = []\n",
    "# simulate at 60Hz to the nearest fixed timestep\n",
    "start_time = sim.get_world_time()\n",
    "while sim.get_world_time() < start_time + dt:\n",
    "    sim.step_physics(1.0 / 60.0)\n",
    "    # remove the object once it passes the occluder center\n",
    "    if obj_id_2 in sim.get_existing_object_ids():\n",
    "        if sim.get_translation(obj_id_2)[1] <= sim.get_translation(occluder_id)[1]:\n",
    "            sim.remove_object(obj_id_2)\n",
    "    observations.append(sim.get_sensor_observations())\n",
    "\n",
    "example_type = \"object permanence\"\n",
    "if make_video:\n",
    "    make_video_cv2(\n",
    "        observations, prefix=example_type, open_vid=show_video, multi_obs=False\n",
    "    )\n",
    "remove_all_objects(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Physical Plausibility Classification\n",
    "# @markdown This example demonstrates a physical plausibility expirement. A sphere\n",
    "# @markdown is dropped onto the back of a couch to roll onto the floor. Optionally,\n",
    "# @markdown an invisible plane is introduced for the sphere to roll onto producing\n",
    "# @markdown non-physical motion.\n",
    "\n",
    "introduce_surface = True  # @param{type:\"boolean\"}\n",
    "\n",
    "remove_all_objects(sim)\n",
    "\n",
    "# add a rolling object\n",
    "obj_attr_mgr = sim.get_object_template_manager()\n",
    "sphere_handle = obj_attr_mgr.get_template_handles(\"uvSphereSolid\")[0]\n",
    "obj_id_1 = sim.add_object_by_handle(sphere_handle)\n",
    "set_object_state_from_agent(sim, obj_id_1, offset=np.array([1.0, 1.6, -1.95]))\n",
    "\n",
    "if introduce_surface:\n",
    "    # optionally add invisible surface\n",
    "    cube_handle = obj_attr_mgr.get_template_handles(\"cube\")[0]\n",
    "    cube_template_cpy = obj_attr_mgr.get_template_by_handle(cube_handle)\n",
    "    # Modify the template.\n",
    "    cube_template_cpy.scale = np.array([1.0, 0.04, 1.0])\n",
    "    surface_is_visible = False  # @param{type:\"boolean\"}\n",
    "    cube_template_cpy.is_visibile = surface_is_visible\n",
    "    # Register the modified template under a new name.\n",
    "    obj_attr_mgr.register_template(cube_template_cpy, \"invisible_surface\")\n",
    "\n",
    "    # Instance and place the surface object from the template.\n",
    "    surface_id = sim.add_object_by_handle(\"invisible_surface\")\n",
    "    set_object_state_from_agent(sim, surface_id, offset=np.array([0.4, 0.88, -1.6]))\n",
    "    sim.set_object_motion_type(habitat_sim.MotionType.STATIC, surface_id)\n",
    "\n",
    "\n",
    "example_type = \"physical plausibility\"\n",
    "observations = simulate(sim, dt=3.0)\n",
    "if make_video:\n",
    "    make_video_cv2(\n",
    "        observations, prefix=example_type, open_vid=show_video, multi_obs=False\n",
    "    )\n",
    "remove_all_objects(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Trajectory Prediction\n",
    "# @markdown This example demonstrates setup of a trajectory prediction task.\n",
    "# @markdown Boxes are placed in a target zone and a sphere is given an initial\n",
    "# @markdown velocity with the goal of knocking the boxes off the counter.\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown Configure Parameters:\n",
    "\n",
    "obj_attr_mgr = sim.get_object_template_manager()\n",
    "remove_all_objects(sim)\n",
    "\n",
    "seed = 2  # @param{type:\"integer\"}\n",
    "random.seed(seed)\n",
    "sim.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# setup agent state manually to face the bar\n",
    "agent_state = sim.agents[0].state\n",
    "agent_state.position = np.array([-1.97496, 0.072447, -2.0894])\n",
    "agent_state.rotation = ut.quat_from_coeffs([0, -1, 0, 0])\n",
    "sim.agents[0].set_state(agent_state)\n",
    "\n",
    "# load the target objects\n",
    "cheezit_handle = obj_attr_mgr.get_template_handles(\"cheezit\")[0]\n",
    "# create range from center and half-extent\n",
    "target_zone = mn.Range3D.from_center(\n",
    "    mn.Vector3(-2.07496, 1.07245, -0.2894), mn.Vector3(0.5, 0.05, 0.1)\n",
    ")\n",
    "num_targets = 9  # @param{type:\"integer\"}\n",
    "for target in range(num_targets):\n",
    "    obj_id = sim.add_object_by_handle(cheezit_handle)\n",
    "    rotate = mn.Quaternion.rotation(mn.Rad(-mn.math.pi_half), mn.Vector3(1.0, 0, 0))\n",
    "    sim.set_rotation(rotate, obj_id)\n",
    "    # sample state from the target zone\n",
    "    if not sample_object_state(sim, obj_id, False, True, 100, target_zone):\n",
    "        sim.remove_object(obj_id)\n",
    "\n",
    "\n",
    "show_target_zone = False  # @param{type:\"boolean\"}\n",
    "if show_target_zone:\n",
    "    # Get and modify the wire cube template from the range\n",
    "    cube_handle = obj_attr_mgr.get_template_handles(\"cubeWireframe\")[0]\n",
    "    cube_template_cpy = obj_attr_mgr.get_template_by_handle(cube_handle)\n",
    "    cube_template_cpy.scale = target_zone.size()\n",
    "    cube_template_cpy.is_collidable = False\n",
    "    # Register the modified template under a new name.\n",
    "    obj_attr_mgr.register_template(cube_template_cpy, \"target_zone\")\n",
    "    # instance and place the object from the template\n",
    "    target_zone_id = sim.add_object_by_handle(\"target_zone\")\n",
    "    sim.set_translation(target_zone.center(), target_zone_id)\n",
    "    sim.set_object_motion_type(habitat_sim.MotionType.STATIC, target_zone_id)\n",
    "    # print(\"target_zone_center = \" + str(sim.get_translation(target_zone_id)))\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ###Ball properties:\n",
    "# load the ball\n",
    "sphere_handle = obj_attr_mgr.get_template_handles(\"uvSphereSolid\")[0]\n",
    "sphere_template_cpy = obj_attr_mgr.get_template_by_handle(sphere_handle)\n",
    "# @markdown Mass:\n",
    "ball_mass = 5.01  # @param {type:\"slider\", min:0.01, max:50.0, step:0.01}\n",
    "sphere_template_cpy.mass = ball_mass\n",
    "obj_attr_mgr.register_template(sphere_template_cpy, \"ball\")\n",
    "\n",
    "ball_id = sim.add_object_by_handle(\"ball\")\n",
    "set_object_state_from_agent(sim, ball_id, offset=np.array([0, 1.4, 0]))\n",
    "\n",
    "# @markdown Initial linear velocity (m/sec):\n",
    "lin_vel_x = -1  # @param {type:\"slider\", min:-10, max:10, step:0.1}\n",
    "lin_vel_y = 1  # @param {type:\"slider\", min:-10, max:10, step:0.1}\n",
    "lin_vel_z = 5  # @param {type:\"slider\", min:0, max:10, step:0.1}\n",
    "initial_linear_velocity = mn.Vector3(lin_vel_x, lin_vel_y, lin_vel_z)\n",
    "sim.set_linear_velocity(initial_linear_velocity, ball_id)\n",
    "\n",
    "# @markdown Initial angular velocity (rad/sec):\n",
    "ang_vel_x = 0  # @param {type:\"slider\", min:-100, max:100, step:0.1}\n",
    "ang_vel_y = 0  # @param {type:\"slider\", min:-100, max:100, step:0.1}\n",
    "ang_vel_z = 0  # @param {type:\"slider\", min:-100, max:100, step:0.1}\n",
    "initial_angular_velocity = mn.Vector3(ang_vel_x, ang_vel_y, ang_vel_z)\n",
    "sim.set_angular_velocity(initial_angular_velocity, ball_id)\n",
    "\n",
    "example_type = \"trajectory prediction\"\n",
    "observations = simulate(sim, dt=3.0)\n",
    "if make_video:\n",
    "    make_video_cv2(\n",
    "        observations, prefix=example_type, open_vid=show_video, multi_obs=False\n",
    "    )\n",
    "remove_all_objects(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embodied Continuous Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example demonstrates setup and excecution of an embodied navigation and interaction scenario. An object and an agent embodied by a rigid locobot mesh are placed randomly on the NavMesh. A path is computed for the agent to reach the object which is executed by a continuous path-following controller. The object is then kinematically gripped by the agent and a second path is computed for the agent to reach a goal location, also executed by a continuous controller. The gripped object is then released and thrown in front of the agent.\n",
    "\n",
    "Note: for a more detailed explanation of the NavMesh see [this(TODO: link)]() tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Select target object from the GUI: { display-mode: \"form\" }\n",
    "\n",
    "build_widget_ui(obj_attr_mgr, prim_attr_mgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Continuous Path Follower Example\n",
    "# @markdown A python Class to provide waypoints along a path given agent states\n",
    "\n",
    "\n",
    "class ContinuousPathFollower(object):\n",
    "    def __init__(self, sim, path, agent_scene_node, waypoint_threshold):\n",
    "        self._sim = sim\n",
    "        self._points = path.points[:]\n",
    "        assert len(self._points) > 0\n",
    "        self._length = path.geodesic_distance\n",
    "        self._node = agent_scene_node\n",
    "        self._threshold = waypoint_threshold\n",
    "        self._step_size = 0.01\n",
    "        self.progress = 0  # geodesic distance -> [0,1]\n",
    "        self.waypoint = path.points[0]\n",
    "\n",
    "        # setup progress waypoints\n",
    "        _point_progress = [0]\n",
    "        _segment_tangents = []\n",
    "        _length = self._length\n",
    "        for ix, point in enumerate(self._points):\n",
    "            if ix > 0:\n",
    "                segment = point - self._points[ix - 1]\n",
    "                segment_length = np.linalg.norm(segment)\n",
    "                segment_tangent = segment / segment_length\n",
    "                _point_progress.append(\n",
    "                    segment_length / _length + _point_progress[ix - 1]\n",
    "                )\n",
    "                # t-1 -> t\n",
    "                _segment_tangents.append(segment_tangent)\n",
    "        self._point_progress = _point_progress\n",
    "        self._segment_tangents = _segment_tangents\n",
    "        # final tangent is duplicated\n",
    "        self._segment_tangents.append(self._segment_tangents[-1])\n",
    "\n",
    "        print(\"self._length = \" + str(self._length))\n",
    "        print(\"num points = \" + str(len(self._points)))\n",
    "        print(\"self._point_progress = \" + str(self._point_progress))\n",
    "        print(\"self._segment_tangents = \" + str(self._segment_tangents))\n",
    "\n",
    "    def pos_at(self, progress):\n",
    "        if progress <= 0:\n",
    "            return self._points[0]\n",
    "        elif progress >= 1.0:\n",
    "            return self._points[-1]\n",
    "\n",
    "        path_ix = 0\n",
    "        for ix, prog in enumerate(self._point_progress):\n",
    "            if prog > progress:\n",
    "                path_ix = ix\n",
    "                break\n",
    "\n",
    "        segment_distance = self._length * (progress - self._point_progress[path_ix - 1])\n",
    "        return (\n",
    "            self._points[path_ix - 1]\n",
    "            + self._segment_tangents[path_ix - 1] * segment_distance\n",
    "        )\n",
    "\n",
    "    def update_waypoint(self):\n",
    "        if self.progress < 1.0:\n",
    "            wp_disp = self.waypoint - self._node.absolute_translation\n",
    "            wp_dist = np.linalg.norm(wp_disp)\n",
    "            node_pos = self._node.absolute_translation\n",
    "            step_size = self._step_size\n",
    "            threshold = self._threshold\n",
    "            while wp_dist < threshold:\n",
    "                self.progress += step_size\n",
    "                self.waypoint = self.pos_at(self.progress)\n",
    "                if self.progress >= 1.0:\n",
    "                    break\n",
    "                wp_disp = self.waypoint - node_pos\n",
    "                wp_dist = np.linalg.norm(wp_disp)\n",
    "\n",
    "\n",
    "def setup_path_visualization(sim, path_follower, vis_ids, vis_samples=100):\n",
    "    sphere_handle = obj_attr_mgr.get_template_handles(\"uvSphereSolid\")[0]\n",
    "    sphere_template_cpy = obj_attr_mgr.get_template_by_handle(sphere_handle)\n",
    "    sphere_template_cpy.scale *= 0.2\n",
    "    obj_attr_mgr.register_template(sphere_template_cpy, \"mini-sphere\")\n",
    "    vis_ids.append(sim.add_object_by_handle(sphere_handle))\n",
    "\n",
    "    for point in path_follower._points:\n",
    "        cp_id = sim.add_object_by_handle(sphere_handle)\n",
    "        sim.set_translation(point, cp_id)\n",
    "        vis_ids.append(cp_id)\n",
    "\n",
    "    for i in range(vis_samples):\n",
    "        cp_id = sim.add_object_by_handle(\"mini-sphere\")\n",
    "        sim.set_translation(path_follower.pos_at(float(i / vis_samples)), cp_id)\n",
    "        vis_ids.append(cp_id)\n",
    "\n",
    "    for id in vis_ids:\n",
    "        sim.set_object_motion_type(habitat_sim.MotionType.KINEMATIC, id)\n",
    "\n",
    "\n",
    "def track_waypoint(waypoint, rs, vc, dt=1.0 / 60.0):\n",
    "    angular_error_threshold = 0.5\n",
    "    max_linear_speed = 1.0\n",
    "    max_turn_speed = 1.0\n",
    "    glob_forward = rs.rotation.transform_vector(mn.Vector3(0, 0, -1.0)).normalized()\n",
    "    glob_right = rs.rotation.transform_vector(mn.Vector3(-1.0, 0, 0)).normalized()\n",
    "    to_waypoint = mn.Vector3(waypoint) - rs.translation\n",
    "    u_to_waypoint = to_waypoint.normalized()\n",
    "    angle_error = float(mn.math.angle(glob_forward, u_to_waypoint))\n",
    "    if angle_error < angular_error_threshold:\n",
    "        # move forward\n",
    "        vc.linear_velocity = mn.Vector3(0, 0, -max_linear_speed)\n",
    "    else:\n",
    "        vc.linear_velocity = mn.Vector3(0)\n",
    "    if angle_error > 0.2:\n",
    "        rot_dir = 1.0\n",
    "        if mn.math.dot(glob_right, u_to_waypoint) < 0:\n",
    "            rot_dir = -1.0\n",
    "        vc.angular_velocity = mn.Vector3(\n",
    "            0, np.clip(rot_dir * angle_error / dt, -max_turn_speed, max_turn_speed), 0\n",
    "        )\n",
    "    else:\n",
    "        vc.angular_velocity = mn.Vector3(0)\n",
    "\n",
    "\n",
    "# grip/release and sync gripped object state kineamtically\n",
    "class ObjectGripper(object):\n",
    "    def __init__(\n",
    "        self, sim, agent_scene_node, end_effector_offset,\n",
    "    ):\n",
    "        self._sim = sim\n",
    "        self._node = agent_scene_node\n",
    "        self._offset = end_effector_offset\n",
    "        self._gripped_obj_id = -1\n",
    "        self._gripped_obj_buffer = 0  # bounding box y dimension offset of the offset\n",
    "\n",
    "    def sync_states(self):\n",
    "        if self._gripped_obj_id != -1:\n",
    "            agent_t = self._node.absolute_transformation_matrix()\n",
    "            agent_t.translation += self._offset + mn.Vector3(\n",
    "                0, self._gripped_obj_buffer, 0.0\n",
    "            )\n",
    "            sim.set_transformation(agent_t, self._gripped_obj_id)\n",
    "\n",
    "    def grip(self, obj_id):\n",
    "        if self._gripped_obj_id != -1:\n",
    "            print(\"Oops, can't carry more than one item.\")\n",
    "            return\n",
    "        self._gripped_obj_id = obj_id\n",
    "        sim.set_object_motion_type(habitat_sim.MotionType.KINEMATIC, obj_id)\n",
    "        object_node = sim.get_object_scene_node(obj_id)\n",
    "        self._gripped_obj_buffer = object_node.cumulative_bb.size_y() / 2.0\n",
    "        self.sync_states()\n",
    "\n",
    "    def release(self):\n",
    "        sim.set_object_motion_type(habitat_sim.MotionType.DYNAMIC, self._gripped_obj_id)\n",
    "        sim.set_linear_velocity(\n",
    "            self._node.absolute_transformation_matrix().transform_vector(\n",
    "                mn.Vector3(0, 0, -1.0)\n",
    "            )\n",
    "            + mn.Vector3(0, 2.0, 0),\n",
    "            self._gripped_obj_id,\n",
    "        )\n",
    "        self._gripped_obj_id = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Embodied Continuous Navigation Example\n",
    "# @markdown This example cell runs the object retrieval task.\n",
    "\n",
    "import faulthandler\n",
    "\n",
    "faulthandler.enable()\n",
    "\n",
    "# @markdown First the Simulator is re-initialized with:\n",
    "# @markdown - a 3rd person camera view\n",
    "# @markdown - modified 1st person sensor placement\n",
    "sim_settings = make_default_settings()\n",
    "# fmt: off\n",
    "sim_settings[\"scene\"] = \"./data/scene_datasets/mp3d/17DRP5sb8fy/17DRP5sb8fy.glb\"  # @param{type:\"string\"}\n",
    "# fmt: on\n",
    "sim_settings[\"sensor_pitch\"] = 0\n",
    "sim_settings[\"sensor_height\"] = 0.6\n",
    "sim_settings[\"color_sensor_3rd_person\"] = True\n",
    "sim_settings[\"depth_sensor_1st_person\"] = True\n",
    "\n",
    "make_simulator_from_settings(sim_settings)\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ### Set other example parameters:\n",
    "seed = 14  # @param {type:\"integer\"}\n",
    "random.seed(seed)\n",
    "sim.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "sim.config.sim_cfg.allow_sliding = True  # @param {type:\"boolean\"}\n",
    "\n",
    "print(sel_file_obj_handle)\n",
    "# load a selected target object and place it on the NavMesh\n",
    "obj_id_1 = sim.add_object_by_handle(sel_file_obj_handle)\n",
    "\n",
    "if not sample_object_state(\n",
    "    sim, obj_id_1, from_navmesh=True, maintain_object_up=True, max_tries=1000\n",
    "):\n",
    "    print(\"Couldn't find an initial object placement. Aborting.\")\n",
    "\n",
    "# load the locobot_merged asset\n",
    "locobot_template_handle = obj_attr_mgr.get_file_template_handles(\"locobot\")[0]\n",
    "\n",
    "# add robot object to the scene with the agent/camera SceneNode attached\n",
    "locobot_id = sim.add_object_by_handle(locobot_template_handle, sim.agents[0].scene_node)\n",
    "\n",
    "# set the agent's body to kinematic since we will be updating position manually\n",
    "sim.set_object_motion_type(habitat_sim.physics.MotionType.KINEMATIC, locobot_id)\n",
    "\n",
    "# create and configure a new VelocityControl structure\n",
    "# Note: this is NOT the object's VelocityControl, so it will not be consumed automatically in sim.step_physics\n",
    "vel_control = habitat_sim.physics.VelocityControl()\n",
    "vel_control.controlling_lin_vel = True\n",
    "vel_control.lin_vel_is_local = True\n",
    "vel_control.controlling_ang_vel = True\n",
    "vel_control.ang_vel_is_local = True\n",
    "\n",
    "# reset observations and robot state\n",
    "sim.set_translation(sim.pathfinder.get_random_navigable_point(), locobot_id)\n",
    "observations = []\n",
    "\n",
    "# get shortest path to the object from the agent position\n",
    "path1 = habitat_sim.ShortestPath()\n",
    "path1.requested_start = sim.get_translation(locobot_id)\n",
    "path1.requested_end = sim.get_translation(obj_id_1)\n",
    "path2 = habitat_sim.ShortestPath()\n",
    "path2.requested_start = path1.requested_end\n",
    "path2.requested_end = sim.pathfinder.get_random_navigable_point()\n",
    "\n",
    "found_path = sim.pathfinder.find_path(path1) and sim.pathfinder.find_path(path2)\n",
    "\n",
    "if not found_path:\n",
    "    print(\"Could not find path to object, aborting!\")\n",
    "vis_ids = []\n",
    "\n",
    "gripper = ObjectGripper(\n",
    "    sim, sim.get_object_scene_node(locobot_id), np.array([0.0, 0.6, 0.0])\n",
    ")\n",
    "continuous_path_follower = ContinuousPathFollower(\n",
    "    sim, path1, sim.get_object_scene_node(locobot_id), waypoint_threshold=0.3\n",
    ")\n",
    "\n",
    "time_step = 1.0 / 30.0\n",
    "for i in range(2):\n",
    "    if i == 1:\n",
    "        gripper.grip(obj_id_1)\n",
    "        continuous_path_follower = ContinuousPathFollower(\n",
    "            sim, path2, sim.get_object_scene_node(locobot_id), waypoint_threshold=0.3\n",
    "        )\n",
    "\n",
    "    show_waypoint_indicators = False  # @param {type:\"boolean\"}\n",
    "    if show_waypoint_indicators:\n",
    "        for id in vis_ids:\n",
    "            sim.remove_object(id)\n",
    "        setup_path_visualization(sim, continuous_path_follower, vis_ids)\n",
    "\n",
    "    # manually control the object's kinematic state via velocity integration\n",
    "    start_time = sim.get_world_time()\n",
    "    max_time = 30.0\n",
    "    while (\n",
    "        continuous_path_follower.progress < 1.0\n",
    "        and sim.get_world_time() - start_time < max_time\n",
    "    ):\n",
    "        continuous_path_follower.update_waypoint()\n",
    "        if show_waypoint_indicators:\n",
    "            sim.set_translation(continuous_path_follower.waypoint, vis_ids[0])\n",
    "\n",
    "        previous_rigid_state = sim.get_rigid_state(locobot_id)\n",
    "\n",
    "        # set velocities based on relative waypoint position/direction\n",
    "        track_waypoint(\n",
    "            continuous_path_follower.waypoint,\n",
    "            previous_rigid_state,\n",
    "            vel_control,\n",
    "            dt=time_step,\n",
    "        )\n",
    "\n",
    "        # manually integrate the rigid state\n",
    "        target_rigid_state = vel_control.integrate_transform(\n",
    "            time_step, previous_rigid_state\n",
    "        )\n",
    "\n",
    "        # snap rigid state to navmesh and set state to object/agent\n",
    "        end_pos = sim.step_filter(\n",
    "            previous_rigid_state.translation, target_rigid_state.translation\n",
    "        )\n",
    "        sim.set_translation(end_pos, locobot_id)\n",
    "        sim.set_rotation(target_rigid_state.rotation, locobot_id)\n",
    "\n",
    "        # Check if a collision occured\n",
    "        dist_moved_before_filter = (\n",
    "            target_rigid_state.translation - previous_rigid_state.translation\n",
    "        ).dot()\n",
    "        dist_moved_after_filter = (end_pos - previous_rigid_state.translation).dot()\n",
    "\n",
    "        # NB: There are some cases where ||filter_end - end_pos|| > 0 when a\n",
    "        # collision _didn't_ happen. One such case is going up stairs.  Instead,\n",
    "        # we check to see if the the amount moved after the application of the filter\n",
    "        # is _less_ than the amount moved before the application of the filter\n",
    "        EPS = 1e-5\n",
    "        collided = (dist_moved_after_filter + EPS) < dist_moved_before_filter\n",
    "\n",
    "        gripper.sync_states()\n",
    "        # run any dynamics simulation\n",
    "        sim.step_physics(time_step)\n",
    "\n",
    "        # render observation\n",
    "        observations.append(sim.get_sensor_observations())\n",
    "\n",
    "# release\n",
    "gripper.release()\n",
    "start_time = sim.get_world_time()\n",
    "while sim.get_world_time() - start_time < 2.0:\n",
    "    sim.step_physics(time_step)\n",
    "    observations.append(sim.get_sensor_observations())\n",
    "\n",
    "# video rendering with embedded 1st person view\n",
    "video_prefix = \"fetch\"\n",
    "if make_video:\n",
    "    make_video_cv2(\n",
    "        observations,\n",
    "        prefix=video_prefix,\n",
    "        open_vid=show_vid,\n",
    "        multi_obs=True,\n",
    "        fps=1.0 / time_step,\n",
    "    )\n",
    "\n",
    "# remove locobot while leaving the agent node for later use\n",
    "sim.remove_object(locobot_id, delete_object_node=False)\n",
    "remove_all_objects(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Features\n",
    "This section contains advanced examples/demos of Habitat-sim interactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initialize Simulator and Load Scene { display-mode: \"form\" }\n",
    "sim_settings = make_default_settings()\n",
    "sim_settings[\"scene\"] = \"./data/scene_datasets/mp3d/17DRP5sb8fy/17DRP5sb8fy.glb\"\n",
    "sim_settings[\"sensor_pitch\"] = 0\n",
    "\n",
    "make_simulator_from_settings(sim_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topic : Asset and Object Customization\n",
    "\n",
    "TODO: a Colab form to edit all fields of an object template and register it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Select target object from the GUI: { display-mode: \"form\" }\n",
    "\n",
    "build_widget_ui(obj_attr_mgr, prim_attr_mgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title 1. Simple File-based Object Template modification. { display-mode: \"form\" }\n",
    "# @markdown Running this will demonstrate how to create objects of varying size from a file-based template by iteratively modifying the template's scale value.  This will also demonstrate how to delete unwanted templates from the library.\n",
    "\n",
    "# Get File-based template for banana using its handle\n",
    "obj_template_handle = \"./data/objects/banana.phys_properties.json\"\n",
    "\n",
    "obj_template = obj_attr_mgr.get_template_by_handle(obj_template_handle)\n",
    "\n",
    "# Add object instantiated by desired template using template handle\n",
    "obj_id = sim.add_object_by_handle(obj_template_handle)\n",
    "\n",
    "# Set desired offset from agent location to place object\n",
    "offset = np.array([-1.2, 1.4, -1.5])\n",
    "# Move object to be in front of the agent\n",
    "set_object_state_from_agent(sim, obj_id, offset=offset)\n",
    "\n",
    "# Templates have editable fields that will directly affect the instanced\n",
    "# objects built from them.  Here we iteratively modify and re-register the\n",
    "# template, instancing a new object each time.\n",
    "# Bigger Bananas!\n",
    "for i in range(5):\n",
    "    # Increase the template scale value (object size)\n",
    "    obj_template.scale *= 1.5\n",
    "    # Make a new handle for the modified template, so we don't overwrite\n",
    "    new_obj_template_handle = obj_template_handle + \"_new_\" + str(i)\n",
    "    # Register modified template with new handle, returns template ID\n",
    "    new_tmplt_ID = obj_attr_mgr.register_template(obj_template, new_obj_template_handle)\n",
    "    # Object creation can occur using template ID or handle\n",
    "    if i % 2 == 0:\n",
    "        # Add another object instantiated by modified template using handle\n",
    "        new_obj = sim.add_object(new_tmplt_ID)\n",
    "    else:\n",
    "        # Add another object instantiated by modified template using handle\n",
    "        new_obj = sim.add_object_by_handle(new_obj_template_handle)\n",
    "    # Move object to the right of previous object\n",
    "    offset[0] += 0.4\n",
    "    set_object_state_from_agent(sim, new_obj, offset=offset)\n",
    "\n",
    "# Clean-up - remove modified templates from template library\n",
    "# Get all modified template handles through keyword search\n",
    "mod_template_handles = obj_attr_mgr.get_template_handles(\"_new_\")\n",
    "# Show number of modified templates\n",
    "print(\n",
    "    \"Before delete, there are {} modified templates.\".format(len(mod_template_handles))\n",
    ")\n",
    "# Display modified template handles\n",
    "print(*mod_template_handles, sep=\"\\n\")\n",
    "# Remove modified templates\n",
    "for handle in mod_template_handles:\n",
    "    obj_attr_mgr.remove_template_by_handle(handle)\n",
    "# Verify removal - get template handles through keyword search\n",
    "mod_template_handles = obj_attr_mgr.get_template_handles(\"_new_\")\n",
    "# Show number of modified templates now\n",
    "print(\n",
    "    \"After deleting added templates, there are {} modified templates.\".format(\n",
    "        len(mod_template_handles)\n",
    "    )\n",
    ")\n",
    "# Display modified template handles\n",
    "print(*mod_template_handles, sep=\"\\n\")\n",
    "\n",
    "example_type = \"Adding edited objects\"\n",
    "observations = simulate(sim, dt=1.0)\n",
    "if make_video:\n",
    "    make_video_cv2(\n",
    "        observations, prefix=example_type, open_vid=show_video, multi_obs=False\n",
    "    )\n",
    "make_clear_all_objects_button()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title 2. All Possible File-based Object Template fields modification. { display-mode: \"form\" }\n",
    "# @markdown This lists all the possible modifiable fields available in an object template, allows for them to be edited and enables creating an object with the new template.\n",
    "\n",
    "# @markdown ###2.1 Fields to edit for a new template\n",
    "# Get a new template\n",
    "new_template = obj_attr_mgr.get_template_by_handle(sel_file_obj_handle)\n",
    "\n",
    "new_template_orig_map = build_dict_of_attrs(new_template)\n",
    "\n",
    "# @markdown The desired mass of the object\n",
    "mass = 1  # @param {type:\"slider\", min:0.1, max:50, step:0.1}\n",
    "new_template.mass = mass\n",
    "\n",
    "# @markdown The x,y,z components for the scale of the object.\n",
    "scale_X = 10  # @param {type:\"slider\", min:0.1, max:10, step:0.1}\n",
    "scale_Y = 10  # @param {type:\"slider\", min:0.1, max:10, step:0.1}\n",
    "scale_Z = 10  # @param {type:\"slider\", min:0.1, max:10, step:0.1}\n",
    "new_template.scale = mn.Vector3(scale_X, scale_Y, scale_Z)\n",
    "\n",
    "# @markdown The x,y,z components for the desired location of the center of mass.\n",
    "com_X = 0  # @param {type:\"slider\", min:-1.0, max:1, step:0.1}\n",
    "com_Y = 0  # @param {type:\"slider\", min:-1.0, max:1, step:0.1}\n",
    "com_Z = 0  # @param {type:\"slider\", min:-1.0, max:1, step:0.1}\n",
    "new_template.com = mn.Vector3(com_X, com_Y, com_Z)\n",
    "\n",
    "# @markdown If true, simulator sets COM as center of bounding box, and ignores any specified COM.\n",
    "compute_COM_from_shape = False  # @param {type:\"boolean\"}\n",
    "new_template.compute_COM_from_shape = compute_COM_from_shape\n",
    "\n",
    "# @markdown Sets the collision margin\n",
    "margin = 0.4  # @param {type:\"slider\", min:0.0, max:10, step:0.1}\n",
    "new_template.margin = margin\n",
    "\n",
    "# @markdown Friction for object contact\n",
    "friction_coefficient = 0.5  # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
    "new_template.friction_coefficient = friction_coefficient\n",
    "\n",
    "# @markdown Fraction of original relative velocity retained by an object after contact.  1 is perfectly elastic contact.\n",
    "restitution_coefficient = 0.3  # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
    "new_template.restitution_coefficient = restitution_coefficient\n",
    "\n",
    "# @markdown Whether the object should be lit via Phong shading.\n",
    "requires_lighting = False  # @param {type:\"boolean\"}\n",
    "new_template.requires_lighting = requires_lighting\n",
    "\n",
    "# @markdown The x,y,z components of the intertia matrix diagonal\n",
    "\n",
    "inertia_X = 1.0  # @param {type:\"slider\", min:0.1, max:10, step:0.1}\n",
    "inertia_Y = 1.0  # @param {type:\"slider\", min:0.1, max:10, step:0.1}\n",
    "inertia_Z = 1.0  # @param {type:\"slider\", min:0.1, max:10, step:0.1}\n",
    "new_template.inertia = mn.Vector3(inertia_X, inertia_Y, inertia_Z)\n",
    "\n",
    "# @markdown Rate of linear momentum dissipation\n",
    "linear_damping = 0.2  # @param {type:\"slider\", min:0.0, max:5.0, step:0.1}\n",
    "new_template.linear_damping = linear_damping\n",
    "\n",
    "# @markdown Rate of angular momentum dissipation\n",
    "angular_damping = 0.2  # @param {type:\"slider\", min:0.0, max:5.0, step:0.1}\n",
    "\n",
    "new_template.angular_damping = angular_damping\n",
    "\n",
    "# @markdown Use bounding box for collision instead of collision mesh.\n",
    "bounding_box_collisions = False  # @param {type:\"boolean\"}\n",
    "new_template.bounding_box_collisions = bounding_box_collisions\n",
    "\n",
    "# @markdown Whether compound collision meshes should be merged into a single convex hull.\n",
    "join_collision_meshes = False  # @param {type:\"boolean\"}\n",
    "new_template.join_collision_meshes = join_collision_meshes\n",
    "\n",
    "# Construct a new handle to save this template with\n",
    "new_template_handle = sel_file_obj_handle + \"_new\"\n",
    "\n",
    "\n",
    "# register new template and get its new id\n",
    "new_template_ID = obj_attr_mgr.register_template(new_template, new_template_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ###2.2 Create an object and display it with the edited template.\n",
    "# Add object instantiated by original template using template handle\n",
    "original_template = obj_attr_mgr.get_template_by_handle(sel_file_obj_handle)\n",
    "orig_obj_id = sim.add_object_by_handle(original_template.handle)\n",
    "\n",
    "# Set desired offset from agent location to place object\n",
    "offset = np.array([-0.5, 1.5, -1.5])\n",
    "# Move object to be in front of the agent\n",
    "set_object_state_from_agent(sim, orig_obj_id, offset=offset)\n",
    "\n",
    "# Add new object instantiated by desired template using template handle\n",
    "obj_id = sim.add_object(new_template_ID)\n",
    "\n",
    "# Set desired offset from agent location to place object\n",
    "offset[0] += 1.0\n",
    "# Move object to be in front of the agent\n",
    "set_object_state_from_agent(sim, obj_id, offset=offset)\n",
    "\n",
    "\n",
    "example_type = \"Adding customized objects\"\n",
    "# Either\n",
    "\n",
    "observations = simulate(sim, dt=2.5)\n",
    "if make_video:\n",
    "    make_video_cv2(\n",
    "        observations, prefix=example_type, open_vid=show_video, multi_obs=False\n",
    "    )\n",
    "make_clear_all_objects_button()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3.0 Simple Primitive-based Object instantiation. { display-mode: \"form\" }\n",
    "# @markdown Habitat-Sim has 6 built-in primitives available (Capsule, Cone, Cube, Cylinder, Icosphere and UVSphere), which are available as both solid and wireframe meshes.  Default objects are synthesized from these primitives automatically and are always available.\n",
    "\n",
    "# Get Primitive-based solid object template handles\n",
    "prim_solid_obj_handles = obj_attr_mgr.get_synth_template_handles(\"solid\")\n",
    "# Get Primitive-based wireframe object template handles\n",
    "prim_wf_obj_handles = obj_attr_mgr.get_synth_template_handles(\"wireframe\")\n",
    "\n",
    "# Set desired offset from agent location to place object\n",
    "offset_solid = np.array([-1.1, 1.6, -1.8])\n",
    "offset_wf = np.array([-1.1, 1.6, -1.0])\n",
    "\n",
    "for i in range(6):\n",
    "    # Create object from template handle\n",
    "    obj_solid_id = sim.add_object_by_handle(prim_solid_obj_handles[i])\n",
    "    obj_wf_id = sim.add_object_by_handle(prim_wf_obj_handles[i])\n",
    "\n",
    "    # Place object in scene relative to agent\n",
    "    set_object_state_from_agent(sim, obj_solid_id, offset=offset_solid)\n",
    "    set_object_state_from_agent(sim, obj_wf_id, offset=offset_wf)\n",
    "\n",
    "    # Move offset for next object\n",
    "    offset_solid[0] += 0.4\n",
    "    offset_wf[0] += 0.4\n",
    "\n",
    "\n",
    "example_type = \"Adding primitive-basedd objects\"\n",
    "# Either\n",
    "observations = simulate(sim, dt=1.0)\n",
    "if make_video:\n",
    "    make_video_cv2(\n",
    "        observations, prefix=example_type, open_vid=show_video, multi_obs=False\n",
    "    )\n",
    "make_clear_all_objects_button()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topic : Motion Tracking Camera\n",
    "\n",
    "While agents in Habitat sim usually act to modify the environment around them, playing the role of active and often primary participants in simulation, there are cases where we may want agents to play the role of the passive observer. In this case we may want an agent to simply \"hold the camera\" and record the results of simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown This example demonstrates updating the agent state to follow the motion of an object during simulation.\n",
    "\n",
    "remove_all_objects(sim)\n",
    "visual_sensor = sim._sensors[\"color_sensor_1st_person\"]\n",
    "initial_sensor_position = np.array(visual_sensor._spec.position)\n",
    "initial_sensor_orientation = np.array(visual_sensor._spec.orientation)\n",
    "# set the color sensor transform to be the agent transform\n",
    "visual_sensor._spec.position = np.array([0, 0, 0])\n",
    "visual_sensor._spec.orientation = np.array([0, 0, 0])\n",
    "visual_sensor._sensor_object.set_transformation_from_spec()\n",
    "\n",
    "# boost the agent off the floor\n",
    "sim.get_agent(0).scene_node.translation += np.array([0, 1.5, 0])\n",
    "observations = []\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ### Set example parameters:\n",
    "seed = 23  # @param {type:\"integer\"}\n",
    "random.seed(seed)\n",
    "sim.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# add an object and position the agent\n",
    "obj_id_1 = sim.add_object_by_handle(sel_file_obj_handle)\n",
    "rand_position = np.random.uniform(\n",
    "    np.array([-0.4, -0.3, -1.0]), np.array([0.4, 0.3, -0.5])\n",
    ")\n",
    "set_object_state_from_agent(sim, obj_id_1, rand_position, ut.random_quaternion())\n",
    "\n",
    "# simulate with updated camera at each frame\n",
    "start_time = sim.get_world_time()\n",
    "while sim.get_world_time() - start_time < 2.0:\n",
    "    sim.step_physics(1.0 / 60.0)\n",
    "    # set agent state to look at object\n",
    "    camera_position = sim.get_agent(0).scene_node.translation\n",
    "    camera_look_at = sim.get_translation(obj_id_1)\n",
    "    sim.get_agent(0).scene_node.rotation = mn.Quaternion.from_matrix(\n",
    "        mn.Matrix4.look_at(\n",
    "            camera_position, camera_look_at, np.array([0, 1.0, 0])  # up\n",
    "        ).rotation()\n",
    "    )\n",
    "    observations.append(sim.get_sensor_observations())\n",
    "\n",
    "# video rendering with embedded 1st person view\n",
    "video_prefix = \"motion tracking\"\n",
    "if make_video:\n",
    "    make_video_cv2(\n",
    "        observations,\n",
    "        prefix=video_prefix,\n",
    "        open_vid=show_video,\n",
    "        multi_obs=False,\n",
    "        fps=60.0,\n",
    "    )\n",
    "\n",
    "# reset the sensor state for other examples\n",
    "visual_sensor._spec.position = initial_sensor_position\n",
    "visual_sensor._spec.orientation = initial_sensor_orientation\n",
    "visual_sensor._sensor_object.set_transformation_from_spec()\n",
    "# put the agent back\n",
    "sim.reset()\n",
    "remove_all_objects(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topic : 3D to 2D Key-point Projection\n",
    "\n",
    "The Habitat-sim visual-sensor API makes it easy to project 3D points into 2D for use cases such as generating ground-truth for image space key-points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ###Display 2D Projection of Object COMs\n",
    "# @markdown First define the projection function using the current state of a chosen\n",
    "# @markdown VisualSensor to set camera parameters and then projects the 3D point.\n",
    "# project a 3D point into 2D image space for a particular sensor\n",
    "def get_2d_point(sim, sensor_name, point_3d):\n",
    "    # get the scene render camera and sensor object\n",
    "    visual_sensor = sim._sensors[sensor_name]\n",
    "    scene_graph = sim.get_active_scene_graph()\n",
    "    scene_graph.set_default_render_camera_parameters(visual_sensor._sensor_object)\n",
    "    render_camera = scene_graph.get_default_render_camera()\n",
    "\n",
    "    # use the camera and projection matrices to transform the point onto the near plane\n",
    "    projected_point_3d = render_camera.projection_matrix.transform_point(\n",
    "        render_camera.camera_matrix.transform_point(point_3d)\n",
    "    )\n",
    "    # convert the 3D near plane point to integer pixel space\n",
    "    point_2d = mn.Vector2(projected_point_3d[0], -projected_point_3d[1])\n",
    "    point_2d = point_2d / render_camera.projection_size()[0]\n",
    "    point_2d += mn.Vector2(0.5)\n",
    "    point_2d *= render_camera.viewport\n",
    "    return mn.Vector2i(point_2d)\n",
    "\n",
    "\n",
    "# @markdown Use this function to compute the projected object center of mass (COM)\n",
    "# @markdown 2D projection and display on the image.\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ### Set example parameters:\n",
    "seed = 27  # @param {type:\"integer\"}\n",
    "random.seed(seed)\n",
    "sim.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "remove_all_objects(sim)\n",
    "\n",
    "# add an object and plot the COM on the image\n",
    "obj_id_1 = sim.add_object_by_handle(sel_file_obj_handle)\n",
    "rand_position = np.random.uniform(\n",
    "    np.array([-0.4, 1.2, -1.0]), np.array([0.4, 1.8, -0.5])\n",
    ")\n",
    "set_object_state_from_agent(sim, obj_id_1, rand_position, ut.random_quaternion())\n",
    "\n",
    "obs = sim.get_sensor_observations()\n",
    "\n",
    "com_2d = get_2d_point(\n",
    "    sim, sensor_name=\"color_sensor_1st_person\", point_3d=sim.get_translation(obj_id_1)\n",
    ")\n",
    "if display:\n",
    "    display_sample(obs[\"color_sensor_1st_person\"], key_points=[com_2d])\n",
    "remove_all_objects(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topic: Configurable Semantic IDs\n",
    "\n",
    "Semantic information can be loaded into the Simulator for scenes directly. This can be provided to models and visualized with the SEMANTIC sensor.\n",
    "\n",
    "What if you want to add new objects to the scene with semantic ids or modify the semantic ids at runtime?\n",
    "\n",
    "Habitat-sim provides three methods for modifying semantic ids which are not provided via vertex colors:\n",
    "\n",
    "1. Configure the semantic id before object instancing via the object template (programmatically or in the json)\n",
    "2. Set the semantic_id of all SceneNodes used to render an object with **Simulator.set_object_semantic_id**.\n",
    "3. Set the **SceneNode.semantic_id** property directly.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ###Configuring Object Semantic IDs:\n",
    "\n",
    "sim_settings = make_default_settings()\n",
    "sim_settings[\"scene\"] = \"./data/scene_datasets/mp3d/17DRP5sb8fy/17DRP5sb8fy.glb\"\n",
    "sim_settings[\"sensor_pitch\"] = 0\n",
    "sim_settings[\"semantic_sensor_1st_person\"] = True\n",
    "\n",
    "make_simulator_from_settings(sim_settings)\n",
    "\n",
    "# @markdown In this example, we load a box asset with each face as a separate\n",
    "# @markdown component with its own SceneNode. We demonstrate the result of modiyfing\n",
    "# @markdown the associated semantic ids via object templates, the Simulator API,\n",
    "# @markdown and the SceneNode property.\n",
    "\n",
    "remove_all_objects(sim)\n",
    "observations = []\n",
    "\n",
    "# @markdown Set the initial object orientation via local Euler angle (degrees):\n",
    "orientation_x = 45  # @param {type:\"slider\", min:-180, max:180, step:1}\n",
    "orientation_y = 45  # @param {type:\"slider\", min:-180, max:180, step:1}\n",
    "orientation_z = 45  # @param {type:\"slider\", min:-180, max:180, step:1}\n",
    "\n",
    "\n",
    "# compose the rotations\n",
    "rotation_x = mn.Quaternion.rotation(mn.Deg(orientation_x), mn.Vector3(1.0, 0, 0))\n",
    "rotation_y = mn.Quaternion.rotation(mn.Deg(orientation_y), mn.Vector3(0, 1.0, 0))\n",
    "rotation_z = mn.Quaternion.rotation(mn.Deg(orientation_z), mn.Vector3(0, 0, 1.0))\n",
    "object_orientation = rotation_z * rotation_y * rotation_x\n",
    "print(object_orientation)\n",
    "\n",
    "# @markdown We can configure the semantic id in the object template. This id\n",
    "# @markdown will be applied to the object (instead of the default 0) upon instantiation:\n",
    "\n",
    "# add a box with default semanticId configured in the template\n",
    "# Note: each face of this box asset is a separate component\n",
    "box_template = habitat_sim.attributes.PhysicsObjectAttributes()\n",
    "box_template.render_asset_handle = str(\n",
    "    os.path.join(data_path, \"test_assets/objects/transform_box.glb\")\n",
    ")\n",
    "box_template.scale = np.array([0.2, 0.2, 0.2])\n",
    "# set the default semantic id for this object template\n",
    "box_template.semantic_id = 10  # @param{type:\"integer\"}\n",
    "box_template_id = obj_attr_mgr.register_template(box_template, \"box\")\n",
    "\n",
    "box_id = sim.add_object(box_template_id)\n",
    "set_object_state_from_agent(\n",
    "    sim, box_id, mn.Vector3(0.0, 1.5, -0.75), orientation=object_orientation\n",
    ")\n",
    "observations.append(sim.get_sensor_observations())\n",
    "\n",
    "# @markdown We can set the semantic id for all components of the object via the Simulator\n",
    "# @markdown API at any time:\n",
    "# override the configured id with a new id\n",
    "box_semantic_id_override = 20  # @param{type:\"integer\"}\n",
    "sim.set_object_semantic_id(box_semantic_id_override, box_id)\n",
    "observations.append(sim.get_sensor_observations())\n",
    "\n",
    "# @markdown We can also set the semantic id for any single SceneNode directly:\n",
    "# set semantic id for specific SceneNode components of the box object\n",
    "box_visual_nodes = sim.get_object_visual_scene_nodes(box_id)\n",
    "box_visual_nodes[6].semantic_id = 3  # @param{type:\"integer\"}\n",
    "box_visual_nodes[7].semantic_id = 4  # @param{type:\"integer\"}\n",
    "observations.append(sim.get_sensor_observations())\n",
    "\n",
    "# display the resulting observations:\n",
    "if display:\n",
    "    for obs in observations:\n",
    "        display_sample(\n",
    "            obs[\"color_sensor_1st_person\"],\n",
    "            semantic_obs=obs[\"semantic_sensor_1st_person\"],\n",
    "        )\n",
    "remove_all_objects(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topic : Generating Clutter with STATIC objects included in NavMesh\n",
    "\n",
    "The NavMesh can be used to place objects on surfaces in the scene. Once objects are placed they can be set to MotionType::STATIC, indiciating that they are not moveable (kinematics and dynamics are disabled for STATIC objects). The NavMesh can then be recomputed including STATIC object meshes in the voxelization.\n",
    "\n",
    "This example demonstrates using the NavMesh to generate a cluttered scene for navigation. In this script we will:\n",
    "\n",
    "- Place objects off the NavMesh\n",
    "- Set them to MotionType::STATIC\n",
    "- Recompute the NavMesh including STATIC objects\n",
    "- Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initialize Simulator and Load Scene { display-mode: \"form\" }\n",
    "# @markdown (load the apartment_1 scene for clutter generation in an open space)\n",
    "sim_settings = make_default_settings()\n",
    "sim_settings[\"scene\"] = \"./data/scene_datasets/habitat-test-scenes/apartment_1.glb\"\n",
    "sim_settings[\"sensor_pitch\"] = 0\n",
    "\n",
    "make_simulator_from_settings(sim_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Select clutter object from the GUI: { display-mode: \"form\" }\n",
    "\n",
    "build_widget_ui(obj_attr_mgr, prim_attr_mgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Clutter Generation Script\n",
    "# @markdown Configure some example parameters:\n",
    "\n",
    "# position the agent\n",
    "sim.agents[0].scene_node.translation = mn.Vector3(0.5, -1.60025, 6.15)\n",
    "print(sim.agents[0].scene_node.rotation)\n",
    "agent_orientation_y = -23  # @param{type:\"integer\"}\n",
    "sim.agents[0].scene_node.rotation = mn.Quaternion.rotation(\n",
    "    mn.Deg(agent_orientation_y), mn.Vector3(0, 1.0, 0)\n",
    ")\n",
    "\n",
    "num_objects = 10  # @param {type:\"slider\", min:0, max:20, step:1}\n",
    "object_scale = 5  # @param {type:\"slider\", min:1.0, max:10.0, step:0.1}\n",
    "\n",
    "# scale up the selected object\n",
    "sel_obj_template_cpy = obj_attr_mgr.get_template_by_handle(sel_file_obj_handle)\n",
    "sel_obj_template_cpy.scale = mn.Vector3(object_scale)\n",
    "obj_attr_mgr.register_template(sel_obj_template_cpy, \"scaled_sel_obj\")\n",
    "\n",
    "# add the selected object\n",
    "sim.navmesh_visualization = True\n",
    "remove_all_objects(sim)\n",
    "fails = 0\n",
    "for obj in range(num_objects):\n",
    "    obj_id_1 = sim.add_object_by_handle(\"scaled_sel_obj\")\n",
    "\n",
    "    # place the object\n",
    "    placement_success = sample_object_state(\n",
    "        sim, obj_id_1, from_navmesh=True, maintain_object_up=True, max_tries=100\n",
    "    )\n",
    "    if not placement_success:\n",
    "        fails += 1\n",
    "        sim.remove_object(obj_id_1)\n",
    "    else:\n",
    "        # set the objects to STATIC so they can be added to the NavMesh\n",
    "        sim.set_object_motion_type(habitat_sim.MotionType.STATIC, obj_id_1)\n",
    "\n",
    "print(\"Placement fails = \" + str(fails) + \"/\" + str(num_objects))\n",
    "\n",
    "# recompute the NavMesh with STATIC objects\n",
    "navmesh_settings = habitat_sim.NavMeshSettings()\n",
    "navmesh_settings.set_defaults()\n",
    "navmesh_success = sim.recompute_navmesh(sim.pathfinder, navmesh_settings, True)\n",
    "\n",
    "# simulate and collect observations\n",
    "example_type = \"clutter generation\"\n",
    "observations = simulate(sim, dt=2.0)\n",
    "if make_video:\n",
    "    make_video_cv2(\n",
    "        observations, prefix=example_type, open_vid=show_video, multi_obs=False\n",
    "    )\n",
    "remove_all_objects(sim)\n",
    "sim.navmesh_visualization = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Advanced Topics:\n",
    "\n",
    "In addition to the topics we covered here, visit the Habitat-sim [python docs](https://aihabitat.org/docs/habitat-sim/) to explore more topics.\n",
    "\n",
    "###Custom Lighting Setups\n",
    "\n",
    "Habitat-sim allows for both Phong and Flat shading options and configurable lighting groups for objects and scenes to be customized. See our [Working with Lights tutorial page](https://aihabitat.org/docs/habitat-sim/lighting-setups.html) to learn more.\n",
    "\n",
    "###Interactive Rigid Objects\n",
    "\n",
    "For more details on the rigid object API, see our [Interactive Rigid Objects tutorial](https://aihabitat.org/docs/habitat-sim/rigid-object-tutorial.html).\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ECCV 2020: Interactivity.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "nb_python//py:percent,colabs//ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
